{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hXhoiRUvnHJ"
   },
   "source": [
    "# Computer vision and deep learning - Laboratory 6\n",
    "\n",
    "In this last laboratory, we will switch our focus from implementing and training neural networks to developing a machine learning application.\n",
    "More specifically you will learn how you can convert your saved torch model into a more portable format using torch script and how you can create a simple demo application for your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v_8pVcLUMVQB",
    "outputId": "c48b4fe3-b4a6-40ad-f3db-cd2085654e57",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/d8/3c/fcfa9f2f9b97e9c38ccb30eb3edef8acbb284487bc7d8f5b98daf01cf757/gradio-4.12.0-py3-none-any.whl.metadata\n",
      "  Using cached gradio-4.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in d:\\anaconda\\lib\\site-packages (from gradio) (22.1.0)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Obtaining dependency information for altair<6.0,>=4.2.0 from https://files.pythonhosted.org/packages/c5/e4/7fcceef127badbb0d644d730d992410e4f3799b295c9964a172f92a469c7/altair-5.2.0-py3-none-any.whl.metadata\n",
      "  Using cached altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fastapi (from gradio)\n",
      "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/d4/e0/d5d6482e992a1892f3a9a62f6a9154944ae5b276e7da1cf92faa02e3a107/fastapi-0.108.0-py3-none-any.whl.metadata\n",
      "  Using cached fastapi-0.108.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software while executing command python setup.py egg_info\n",
      "ERROR: Could not install packages due to an OSError: [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software\n",
      "\n",
      "ERROR: Could not find a version that satisfies the requirement torchscript (from versions: none)\n",
      "ERROR: No matching distribution found for torchscript\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n",
    "!pip install torchscript"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8spOviRwB3Uz"
   },
   "source": [
    "## Converting your model into portable TorchScript binaries\n",
    "\n",
    "\n",
    "``TorchScript`` allows you to create serializable and optimizable models from PyTorch code and then use them in a process where there is no Python dependency.\n",
    "\n",
    "\n",
    "When deploying our module in production systems, we might need to run the model using another programming language (not Python) and even on mobile or embedded devices. In addition, we need a more lightweight environment than the development one.\n",
    "\n",
    "\n",
    "Until now, when training a model we've saved checkpoints and reloaded the weights when needed into the development environment. As the name suggests, the checkpoints contain additional information (such as optimizer states) which allows you to resume the training process. However, all this information is not required during inference.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "``Torchscript`` allows you to create a lightweight and independent model artifact suitable for runtime via two different techniques: scripting and tracing. They are both used to convert a PyTorch model into a more optimized or deployable form.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Tracing involves capturing a model's execution trace by passing example inputs through the model and recording the operations executed. This creates a TorchScript representation of the model based on the traced operations. However, tracing might not capture all dynamic aspects of the model, especially if the model's behavior changes dynamically based on input data or control flow operations. Tracing is more focused on capturing the specific operations executed with example inputs, which might be more efficient but might not cover all dynamic behaviors of complex models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Scripting, on the other hand, refers to converting a PyTorch model (built using PyTorch's dynamic computation graph with Python control flow, such as loops and if statements) into a TorchScript. This involves representing the model as a static computation graph that can be executed independently of Python. Scripting allows the model to be saved and run in environments where a Python interpreter might not be available. Scripting captures the entire model logic and can handle more complex models with Pythonic control flow, making it more flexible but potentially more complex.\n",
    "\n",
    "\n",
    "Both techniques aim to transform PyTorch models into TorchScript representations, making them efficient for deployment in various environments or for optimized execution, albeit with different approaches. The choice between scripting and tracing depends on the specific use case, model complexity, and deployment requirements.\n",
    "\n",
    "You can check out the [documentation](https://pytorch.org/docs/stable/jit.html) for further details on ``TorchScript``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26fLZ04xnXyA"
   },
   "source": [
    "Below you have an example that demonstrates the conversion of a pre-trained ResNet-18 model from torchvision into a TorchScript and then loading and using the saved TorchScript model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2gPwGpavl7C"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Create a sample input tensor (change according to your model's input requirements)\n",
    "example_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Script the model\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Save the scripted model to a file\n",
    "scripted_model.save(\"scripted_resnet18.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBYTvmFYr9Fz"
   },
   "source": [
    "The main steps of the process are:\n",
    "- load the pre-trained model and set it to evaluation mode with model.eval().\n",
    "- create a sample input tensor (example_input) that matches the expected input shape of the model.\n",
    "- use ```torch.jit.script()``` to convert the model into a TorchScript representation.\n",
    "- save the scripted model to a file using ```scripted_model.save()``` for later use or deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eMTs3vbsV56"
   },
   "source": [
    "Now, let's see how you can use the scripted model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TO58HkOtsZXx"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import torchvision\n",
    "from torchvision.models import  ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved TorchScript model\n",
    "model = torch.jit.load(\"scripted_resnet18.pt\")\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "image_url = 'https://images.unsplash.com/photo-1611267254323-4db7b39c732c?q=80&w=1000&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8M3x8Y3V0ZSUyMGNhdHxlbnwwfHwwfHx8MA%3D%3D'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # run the scripted model\n",
    "    output = model(input_batch)\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "\n",
    "class_names = weights.meta[\"categories\"]\n",
    "\n",
    "# Get the top 5 predictions\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "# Display top 5 predicted classes and their probabilities\n",
    "for i in range(top5_prob.size(0)):\n",
    "    class_idx = top5_catid[i].item()\n",
    "    print(f\"Prediction: {class_names[class_idx]}, Probability: {top5_prob[i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oaEEfd1NOxyk"
   },
   "source": [
    "Optionally, you can also save the torchscript binary into ```wandb```. In this way, you will have a connection link between the model that is running in production and the training runs that you logged during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E-btbzsULYXM"
   },
   "source": [
    "# Creating a simple UI with gradio\n",
    "\n",
    "\n",
    "[Gradio](https://www.gradio.app/docs/interface) is an open-source Python library used for creating customizable UI components for machine learning models with just a few lines of code. It greatly simplifies the process of building web-based interfaces to interact with ML models without requiring extensive knowledge of web development and allows you to quickly build an MVP and get feedback from the users.\n",
    "\n",
    "\n",
    "To get an application running, you just need to specify three parameters:\n",
    "1. the function to wrap the interface around.\n",
    "2. what are the desired input components?\n",
    "3. what are the desired output components?\n",
    "\n",
    "\n",
    "This is achieved through the ``gradio.Interface`` class, the central component in gradio, responsible for creating the user interface for your machine learning model.\n",
    "\n",
    "\n",
    "```\n",
    "import gradio as gr\n",
    "demo = gr.Interface(fn=image_classifier,\n",
    "                    inputs=\"image\",\n",
    "                    outputs=\"label\")\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "Once you've defined the gr.Interface, the launch() method is used to start the interface, making it accessible through a web browser.\n",
    "\n",
    "\n",
    "```\n",
    "demo.launch()\n",
    "```\n",
    "\n",
    "\n",
    "When the launch method is called, ```gradio``` launches a simple web server that serves the demo. If you specify ```share=True``` when calling the launch function, ```gradio``` will create a public link Can also be used to create a public link used by anyone to access the demo from their browser.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXflaukmmWPh"
   },
   "source": [
    "## Simple UI for image classification in gradio\n",
    "\n",
    "Below you have an example of how you could use ```gradio``` to create a simple UI for an image classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "fpXMo37vMUYL",
    "outputId": "99a5b2a7-9173-42fc-96e7-ba2aba458039"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://239025952eec3aa653.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://239025952eec3aa653.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def softmax(x):\n",
    "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
    "\n",
    "\n",
    "def classify_image(img):\n",
    "    # TODO run a classification model to get the class scores\n",
    "    prediction = softmax(np.random.randn(10, ))\n",
    "    confidences = {CLASSES[i]: float(prediction[i]) for i in range(len(CLASSES))}\n",
    "    return confidences\n",
    "\n",
    "ui = gr.Interface(fn=classify_image,\n",
    "             inputs=gr.Image(),\n",
    "             outputs=gr.Label(num_top_classes=3),\n",
    "             # TODO replace example1.png example2.png with some images from your device\n",
    "            #examples=['example1.png', 'example2.png']\n",
    "          )\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKXtFC0_hkcm"
   },
   "source": [
    "## Accessing the webcam with gradio\n",
    "\n",
    "In the example below, you have an example in which you take the input images from your webcam.\n",
    "The function wrapped by gradio uses a mask to blur the input image outside that mask. If you plan to do background blurring, the mask could be the segmentation mask predicted by your model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 626
    },
    "id": "HzC5frqLhk1I",
    "outputId": "c1e40a6a-c49b-4a93-f914-fce4eeb17341"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
      "\n",
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "Running on public URL: https://f796847fc627c393e3.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f796847fc627c393e3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "def blur_background(input_image):\n",
    "    input_image = cv2.cvtColor(input_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Generate a blank mask\n",
    "    # TODO your code here: call a segmentation model to get predicted mask\n",
    "    mask = np.zeros_like(input_image)\n",
    "\n",
    "    # for demo purposes, we are going to create a random segmentation mask\n",
    "    #  just a circular blob centered in the middle of the image\n",
    "    center_x, center_y = mask.shape[1] // 2, mask.shape[0] // 2\n",
    "    cv2.circle(mask, (center_x, center_y), 100, (255, 255, 255), -1)\n",
    "\n",
    "    # Convert the mask to grayscale\n",
    "    mask_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "    mask_gray = mask_gray[:,:,np.newaxis]\n",
    "\n",
    "\n",
    "\n",
    "    # apply a strong Gaussian blur to the areas outside the mask\n",
    "    blurred = cv2.GaussianBlur(input_image, (51, 51), 0)\n",
    "    result = np.where(mask_gray, input_image, blurred)\n",
    "\n",
    "    # Convert the result back to RGB format for Gradio\n",
    "    result = cv2.cvtColor(result, cv2.COLOR_BGR2RGB)\n",
    "    return result\n",
    "\n",
    "\n",
    "ui = gr.Interface(\n",
    "    fn=blur_background,\n",
    "    inputs=gr.Image(sources=[\"webcam\"]),\n",
    "    outputs=\"image\",\n",
    "    title=\"Image segmentation demo!\"\n",
    "\n",
    ")\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhaGtNlJj1Wr"
   },
   "source": [
    "## Laboratory assignment\n",
    "\n",
    "\n",
    "Now you have all the knowledge required to build your own ML semantic segmentation application.\n",
    "\n",
    "\n",
    "1. First use ```torchscript``` to obtain a model binary.\n",
    "2. Using gradio, create a simple application that uses the semantic segmentation that you developed. Feel free to define the scope and the functional requirements of your app.\n",
    "3. __[Optional, independent work]__ Use a serverless cloud function on [AWS Lambda](https://aws.amazon.com/lambda/) (this requires an account on Amazon AWS and you need to provide the details of a credit card) to run the prediction and get the results.\n",
    "\n",
    "\n",
    "Congratulations, you've just completed all the practical work for Computer Vision and Deep Learning!\n",
    "May your data always be clean, your models accurate, and your code bug-free!\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
