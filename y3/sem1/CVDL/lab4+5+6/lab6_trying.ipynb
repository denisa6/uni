{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa5f5d01-25b9-4aa6-bdd2-1d47145bcafb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Obtaining dependency information for gradio from https://files.pythonhosted.org/packages/d8/3c/fcfa9f2f9b97e9c38ccb30eb3edef8acbb284487bc7d8f5b98daf01cf757/gradio-4.12.0-py3-none-any.whl.metadata\n",
      "  Using cached gradio-4.12.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: aiofiles<24.0,>=22.0 in d:\\anaconda\\lib\\site-packages (from gradio) (22.1.0)\n",
      "Collecting altair<6.0,>=4.2.0 (from gradio)\n",
      "  Obtaining dependency information for altair<6.0,>=4.2.0 from https://files.pythonhosted.org/packages/c5/e4/7fcceef127badbb0d644d730d992410e4f3799b295c9964a172f92a469c7/altair-5.2.0-py3-none-any.whl.metadata\n",
      "  Using cached altair-5.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting fastapi (from gradio)\n",
      "  Obtaining dependency information for fastapi from https://files.pythonhosted.org/packages/d4/e0/d5d6482e992a1892f3a9a62f6a9154944ae5b276e7da1cf92faa02e3a107/fastapi-0.108.0-py3-none-any.whl.metadata\n",
      "  Using cached fastapi-0.108.0-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Using cached ffmpy-0.3.1.tar.gz (5.5 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: Error [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software while executing command python setup.py egg_info\n",
      "ERROR: Could not install packages due to an OSError: [WinError 225] Operation did not complete successfully because the file contains a virus or potentially unwanted software\n",
      "\n",
      "ERROR: Could not find a version that satisfies the requirement torchscript (from versions: none)\n",
      "ERROR: No matching distribution found for torchscript\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio\n",
    "!pip install torchscript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35dcd20-765f-414a-92e7-11940b060b32",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] The specified procedure could not be found'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "D:\\anaconda\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\Denisa/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:40<00:00, 1.17MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Create a sample input tensor (change according to your model's input requirements)\n",
    "example_input = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "# Script the model\n",
    "scripted_model = torch.jit.script(model)\n",
    "\n",
    "# Save the scripted model to a file\n",
    "scripted_model.save(\"scripted_resnet18.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da9a26f1-43b4-4508-a00b-5d5914cb8382",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: tabby, Probability: 0.7669\n",
      "Prediction: tiger cat, Probability: 0.1399\n",
      "Prediction: Egyptian cat, Probability: 0.0867\n",
      "Prediction: Persian cat, Probability: 0.0024\n",
      "Prediction: lynx, Probability: 0.0012\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import torchvision\n",
    "from torchvision.models import  ResNet18_Weights\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "\n",
    "# Load the saved TorchScript model\n",
    "model = torch.jit.load(\"scripted_resnet18.pt\")\n",
    "\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "\n",
    "image_url = 'https://images.unsplash.com/photo-1611267254323-4db7b39c732c?q=80&w=1000&auto=format&fit=crop&ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxzZWFyY2h8M3x8Y3V0ZSUyMGNhdHxlbnwwfHwwfHx8MA%3D%3D'\n",
    "response = requests.get(image_url)\n",
    "image = Image.open(BytesIO(response.content)).convert('RGB')\n",
    "\n",
    "\n",
    "input_tensor = preprocess(image)\n",
    "input_batch = input_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    # run the scripted model\n",
    "    output = model(input_batch)\n",
    "\n",
    "weights = ResNet18_Weights.DEFAULT\n",
    "\n",
    "class_names = weights.meta[\"categories\"]\n",
    "\n",
    "# Get the top 5 predictions\n",
    "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "\n",
    "# Display top 5 predicted classes and their probabilities\n",
    "for i in range(top5_prob.size(0)):\n",
    "    class_idx = top5_catid[i].item()\n",
    "    print(f\"Prediction: {class_names[class_idx]}, Probability: {top5_prob[i].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1505fb-a168-4884-bc5a-83810a3a9cea",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gradio as gr\n",
    "\n",
    "CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "def softmax(x):\n",
    "    return(np.exp(x - np.max(x)) / np.exp(x - np.max(x)).sum())\n",
    "\n",
    "\n",
    "def classify_image(img):\n",
    "    img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "    img = transforms.ToTensor()(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "      prediction = torch.nn.functional.softmax(model(img)[0], dim=0)\n",
    "    weights = ResNet18_Weights.DEFAULT\n",
    "    class_names = weights.meta[\"categories\"]\n",
    "    confidences = {class_names[i]: float(prediction[i]) for i in range(len(class_names))}\n",
    "    return confidences\n",
    "\n",
    "ui = gr.Interface(fn=classify_image,\n",
    "             inputs=gr.Image(),\n",
    "             outputs=gr.Label(num_top_classes=3),\n",
    "             # TODO replace example1.png example2.png with some images from your device\n",
    "             examples=['example1.png', 'example2.png']\n",
    "          )\n",
    "ui.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b29c7f-eddf-4c16-a483-4e10208166dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
